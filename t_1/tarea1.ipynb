{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b720837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7321126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "pi = tf.constant(np.pi, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67f1ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TensorFlow: 2.19.0\n",
      "Construido con CUDA: True\n",
      "GPUs disponibles: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "\n",
    "# Comprueba si TensorFlow fue compilado con soporte para CUDA (GPU)\n",
    "print(\"Construido con CUDA:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Lista los dispositivos físicos de tipo GPU que TensorFlow puede ver\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs disponibles: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "    # Opcional: Habilitar el crecimiento de memoria para evitar que TF acapare toda la VRAM\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"¡No se encontró ninguna GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9cf938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fun_1(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise ValueError(f\"Input must be a pytorch tensor but is {type(x)}\")\n",
    "    if (2,) != x.shape:\n",
    "        raise ValueError(f\"Input must be of shape (2, ) but has shape {x.shape}\")\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "    y = x + torch.randint(4, 20, size=(2, ), generator=g)\n",
    "    return sum(y**2)\n",
    "\n",
    "def torch_fun_2(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise ValueError(f\"Input must be a pytorch tensor but is {type(x)}\")\n",
    "    if (10,) != x.shape:\n",
    "        raise ValueError(f\"Input must be of shape (10, ) but has shape {x.shape}\")\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "    y = x + torch.randint(4, 20, size=(10, ), generator=g)\n",
    "    return sum(y**2)\n",
    "\n",
    "def tf_fun_1(x):\n",
    "    if not isinstance(x, tf.Variable):\n",
    "        raise ValueError(f\"Input must be a tensorflow Variable but is {type(x)}\")\n",
    "    if (2,) != x.shape:\n",
    "        raise ValueError(f\"Input must be of shape (2, ) but has shape {x.shape}\")\n",
    "    g = tf.random.Generator.from_seed(42)\n",
    "    y = x + g.uniform(shape=(2,), minval=4, maxval=20, dtype=tf.float32)\n",
    "    return sum(y**2)\n",
    "\n",
    "def tf_fun_2(x):\n",
    "    if not isinstance(x, tf.Variable):\n",
    "        raise ValueError(f\"Input must be a tensorflow Variable but is {type(x)}\")\n",
    "    if (10,) != x.shape:\n",
    "        raise ValueError(f\"Input must be of shape (10, ) but has shape {x.shape}\")\n",
    "    \n",
    "    g = tf.random.Generator.from_seed(42)\n",
    "    y = x + g.uniform(shape=(10,), minval=4, maxval=20, dtype=tf.float32)\n",
    "    return sum(y**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "971fc85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_torch_fun1(f):\n",
    "    print(\"Testing Torch 1\")\n",
    "    \"\"\"\n",
    "    Compite con varios optimizadores para encontrar el arg min f y devuelve\n",
    "    el tensor del optimizador más rápido.\n",
    "    \"\"\"\n",
    "    # Parámetros de la competencia interna\n",
    "    tolerance = 1e-4\n",
    "    max_steps = 1000 # Aumentamos los pasos para dar oportunidad a los más lentos\n",
    "    \n",
    "    # Estructura para guardar el resultado de cada optimizador\n",
    "    results = {}\n",
    "    \n",
    "    learning_rates = {\"SGD\": 0.01, \"Momentum\": 0.01, \"Adagrad\": 0.1, \"RMSprop\": 0.01, \"Adam\": 0.1}\n",
    "    optimizers_to_test = [\"SGD\", \"Momentum\", \"Adagrad\", \"RMSprop\", \"Adam\"]\n",
    "\n",
    "    print(\"--- Iniciando competencia en optimize_torch_fun1 ---\")\n",
    "    for opt_name in optimizers_to_test:\n",
    "        # Reiniciamos la variable 'x' para cada optimizador, para una competencia justa\n",
    "        x = torch.zeros(2, requires_grad=True)\n",
    "        lr = learning_rates.get(opt_name, 0.1)\n",
    "\n",
    "        # Seleccionar el optimizador\n",
    "        if opt_name == \"SGD\": optimizer = torch.optim.SGD([x], lr=lr)\n",
    "        elif opt_name == \"Momentum\": optimizer = torch.optim.SGD([x], lr=lr, momentum=0.9)\n",
    "        elif opt_name == \"Adagrad\": optimizer = torch.optim.Adagrad([x], lr=lr)\n",
    "        elif opt_name == \"RMSprop\": optimizer = torch.optim.RMSprop([x], lr=lr)\n",
    "        elif opt_name == \"Adam\": optimizer = torch.optim.Adam([x], lr=lr)\n",
    "        else: continue\n",
    "\n",
    "        # Bucle de optimización hasta la convergencia\n",
    "        for step in range(1, max_steps + 1):\n",
    "            optimizer.zero_grad()\n",
    "            loss = f(x)\n",
    "            if loss.item() < tolerance:\n",
    "                # ¡Importante! Guardamos el número de pasos y una copia del tensor final\n",
    "                results[opt_name] = {\"steps\": step, \"final_x\": x.detach().clone()}\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else: # Si el bucle termina sin converger\n",
    "            results[opt_name] = {\"steps\": max_steps, \"final_x\": x.detach().clone()}\n",
    "\n",
    "    # --- Lógica para decidir el ganador ---\n",
    "    \n",
    "    # Filtramos los que sí convergieron\n",
    "    converged_optimizers = {name: data for name, data in results.items() if data['steps'] < max_steps}\n",
    "    \n",
    "    if converged_optimizers:\n",
    "        # Encontramos el nombre del optimizador más rápido (menos pasos)\n",
    "        best_optimizer_name = min(converged_optimizers, key=lambda name: converged_optimizers[name]['steps'])\n",
    "        print(f\"-> Ganador de la competencia: {best_optimizer_name} con {results[best_optimizer_name]['steps']} pasos.\")\n",
    "        # Recuperamos el tensor del optimizador más rápido y lo devolvemos\n",
    "        return results[best_optimizer_name]['final_x']\n",
    "    else:\n",
    "        # Fallback: si ninguno convergió, devolvemos el resultado de Adam (suele ser el más robusto)\n",
    "        print(\"-> Ningún optimizador convergió, devolviendo el resultado de Adam.\")\n",
    "        return results['Adam']['final_x']\n",
    "\n",
    "def optimize_torch_fun2(f):\n",
    "    print(\"Testing Torch 2\")\n",
    "    \"\"\"\n",
    "    Encuentra arg min f utilizando PyTorch.\n",
    "\n",
    "    Args:\n",
    "        f: una función que recibe un tensor de PyTorch de forma (10,) y devuelve un float.\n",
    "    \n",
    "    Return: un tensor de PyTorch de forma (10,)\n",
    "    \"\"\"\n",
    "        # Parámetros de la competencia interna\n",
    "    tolerance = 1e-4\n",
    "    max_steps = 1000 # Aumentamos los pasos para dar oportunidad a los más lentos\n",
    "    \n",
    "    # Estructura para guardar el resultado de cada optimizador\n",
    "    results = {}\n",
    "    \n",
    "    learning_rates = {\"SGD\": 0.01, \"Momentum\": 0.01, \"Adagrad\": 0.1, \"RMSprop\": 0.01, \"Adam\": 0.1}\n",
    "    optimizers_to_test = [\"SGD\", \"Momentum\", \"Adagrad\", \"RMSprop\", \"Adam\"]\n",
    "\n",
    "    print(\"--- Iniciando competencia en optimize_torch_fun2 ---\")\n",
    "    for opt_name in optimizers_to_test:\n",
    "        # Reiniciamos la variable 'x' para cada optimizador, para una competencia justa\n",
    "        x = torch.zeros(10, requires_grad=True)\n",
    "        lr = learning_rates.get(opt_name, 0.1)\n",
    "\n",
    "        # Seleccionar el optimizador\n",
    "        if opt_name == \"SGD\": optimizer = torch.optim.SGD([x], lr=lr)\n",
    "        elif opt_name == \"Momentum\": optimizer = torch.optim.SGD([x], lr=lr, momentum=0.9)\n",
    "        elif opt_name == \"Adagrad\": optimizer = torch.optim.Adagrad([x], lr=lr)\n",
    "        elif opt_name == \"RMSprop\": optimizer = torch.optim.RMSprop([x], lr=lr)\n",
    "        elif opt_name == \"Adam\": optimizer = torch.optim.Adam([x], lr=lr)\n",
    "        else: continue\n",
    "\n",
    "        # Bucle de optimización hasta la convergencia\n",
    "        for step in range(1, max_steps + 1):\n",
    "            optimizer.zero_grad()\n",
    "            loss = f(x)\n",
    "            if loss.item() < tolerance:\n",
    "                # ¡Importante! Guardamos el número de pasos y una copia del tensor final\n",
    "                results[opt_name] = {\"steps\": step, \"final_x\": x.detach().clone()}\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else: # Si el bucle termina sin converger\n",
    "            results[opt_name] = {\"steps\": max_steps, \"final_x\": x.detach().clone()}\n",
    "\n",
    "    # --- Lógica para decidir el ganador ---\n",
    "    \n",
    "    # Filtramos los que sí convergieron\n",
    "    converged_optimizers = {name: data for name, data in results.items() if data['steps'] < max_steps}\n",
    "    \n",
    "    if converged_optimizers:\n",
    "        # Encontramos el nombre del optimizador más rápido (menos pasos)\n",
    "        best_optimizer_name = min(converged_optimizers, key=lambda name: converged_optimizers[name]['steps'])\n",
    "        print(f\"-> Ganador de la competencia: {best_optimizer_name} con {results[best_optimizer_name]['steps']} pasos.\")\n",
    "        # Recuperamos el tensor del optimizador más rápido y lo devolvemos\n",
    "        return results[best_optimizer_name]['final_x']\n",
    "    else:\n",
    "        # Fallback: si ninguno convergió, devolvemos el resultado de Adam (suele ser el más robusto)\n",
    "        print(\"-> Ningún optimizador convergió, devolviendo el resultado de Adam.\")\n",
    "        return results['Adam']['final_x']\n",
    "\n",
    "\n",
    "def _optimize_tf(f, shape, tolerance, max_steps):\n",
    "    \"\"\"Función auxiliar que ejecuta la competencia en modo Eager.\"\"\"\n",
    "    results = {}\n",
    "    learning_rates = {\"SGD\": 0.01, \"Momentum\": 0.01, \"Adagrad\": 0.1, \"RMSprop\": 0.01, \"Adam\": 0.1}\n",
    "    optimizers_to_test = [\"SGD\", \"Momentum\", \"Adagrad\", \"RMSprop\", \"Adam\"]\n",
    "\n",
    "    print(\"--- Iniciando competencia en _optimize_tf ---\")\n",
    "    for opt_name in optimizers_to_test:\n",
    "        x = tf.Variable(tf.zeros(shape), dtype=tf.float32)\n",
    "        lr = learning_rates.get(opt_name, 0.1)\n",
    "\n",
    "        if opt_name == \"SGD\": optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        elif opt_name == \"Momentum\": optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        elif opt_name == \"Adagrad\": optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "        elif opt_name == \"RMSprop\": optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        elif opt_name == \"Adam\": optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else: continue\n",
    "\n",
    "        for step in range(1, max_steps + 1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = f(x)\n",
    "            \n",
    "            if loss < tolerance:\n",
    "                results[opt_name] = {\"steps\": step, \"final_x\": x}\n",
    "                break\n",
    "            \n",
    "            gradients = tape.gradient(loss, [x])\n",
    "            \n",
    "            if gradients[0] is not None:\n",
    "                optimizer.apply_gradients(zip(gradients, [x]))\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            results[opt_name] = {\"steps\": max_steps, \"final_x\": x}\n",
    "\n",
    "    # ----- ¡CAMBIOS AQUÍ! Se añaden los prints solicitados -----\n",
    "    converged_optimizers = {name: data for name, data in results.items() if data['steps'] < max_steps}\n",
    "    if converged_optimizers:\n",
    "        best_optimizer_name = min(converged_optimizers, key=lambda name: converged_optimizers[name]['steps'])\n",
    "        print(f\"-> Ganador de la competencia: {best_optimizer_name} con {results[best_optimizer_name]['steps']} pasos.\")\n",
    "        return results[best_optimizer_name]['final_x']\n",
    "    else:\n",
    "        print(\"-> Ningún optimizador convergió, devolviendo el resultado de Adam.\")\n",
    "        return results['Adam']['final_x']\n",
    "\n",
    "def optimize_tf_fun1(f):\n",
    "    return _optimize_tf(f, shape=(2,), tolerance=1e-4, max_steps=1000)\n",
    "\n",
    "def optimize_tf_fun2(f):\n",
    "    return _optimize_tf(f, shape=(10,), tolerance=1e-4, max_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "010b9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Torch 1\n",
      "--- Iniciando competencia en optimize_torch_fun1 ---\n",
      "-> Ganador de la competencia: Momentum con 38 pasos.\n",
      "Testing Torch 2\n",
      "--- Iniciando competencia en optimize_torch_fun2 ---\n",
      "-> Ganador de la competencia: Momentum con 108 pasos.\n",
      "--- Iniciando competencia en _optimize_tf ---\n",
      "-> Ganador de la competencia: Momentum con 107 pasos.\n",
      "Optimizer used 37.966572761535645s, but only 30 are allowed\n",
      "--- Iniciando competencia en _optimize_tf ---\n",
      "-> Ganador de la competencia: Momentum con 108 pasos.\n",
      "Optimizer used 58.91860055923462s, but only 30 are allowed\n"
     ]
    }
   ],
   "source": [
    "def score_fun(fun, optimization_fun, optimal_score, minimal_score_for_points, timeout):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # run optimizer\n",
    "        t_start = time()\n",
    "        params = optimization_fun(fun)\n",
    "        runtime = time() - t_start\n",
    "        if runtime > timeout:\n",
    "            print(f\"Optimizer used {runtime}s, but only {timeout} are allowed\")\n",
    "\n",
    "        # determine performance\n",
    "        funval = fun(params)\n",
    "        if funval > minimal_score_for_points:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return np.round(float((100 * (minimal_score_for_points - funval) / (minimal_score_for_points - optimal_score))), 1)\n",
    "        \n",
    "    except Exception:\n",
    "        raise\n",
    "\n",
    "# run optimizer on first task\n",
    "leaderboard_scores = []\n",
    "\n",
    "scores = []\n",
    "for name, obj_fun, optimizer_fun, least_possible_fun_val, fun_val_to_receive_points in [\n",
    "    (\"PyTorch Function 1\", torch_fun_1, optimize_torch_fun1, 0, 100),\n",
    "    (\"PyTorch Function 2\", torch_fun_2, optimize_torch_fun2, 0, 100),\n",
    "    (\"TensorFlow Function 1\", tf_fun_1, optimize_tf_fun1, 0, 100),\n",
    "    (\"TensorFlow Function 2\", tf_fun_2, optimize_tf_fun2, 0, 100)\n",
    "]:\n",
    "    score = score_fun(obj_fun, optimizer_fun, least_possible_fun_val, fun_val_to_receive_points, timeout=30)\n",
    "    leaderboard_scores.append({\n",
    "        \"name\": name,\n",
    "        \"value\": score\n",
    "    })\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5418f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(100.0), np.float64(100.0), np.float64(100.0), np.float64(100.0)]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
